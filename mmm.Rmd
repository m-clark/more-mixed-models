---
title: "More Mixed Models"
output:
  html_document: 
    highlight: pygments
    theme: sandstone
    toc: no
    toc_depth: 3
  html_notebook:
    editor_options:
      chunk_output_type: console
---

```{r html_setup, include=FALSE}
knitr::opts_chunk$set(
  echo=T, 
  message = F, 
  warning = F, 
  comment = NA,
  R.options=list(width=120), 
  cache.rebuild=F, 
  cache=T,
  fig.align='center', 
  fig.width = 3,
  fig.asp = .7,
  dev = 'svg', 
  dev.args=list(bg = 'transparent')
)
```

# Beyond `lme4` {.tabset}

## Introduction 

Mixed models are an extremely useful modeling tool for situations in which there is some dependency among observations in the data, where the correlation typically arises from the observations being clustered in some way. For example, it is quite common to have data in which we have repeated measurements for the units of observation, or in which the units of observation are otherwise grouped together (e.g. students within school, cities within geographic region). While there are different ways to approach such a situation, mixed models are a very common and powerful tool to do so. In addition, they have ties to other statistical approaches that further expand their applicability.

The following depicts a simple mixed model for an observation.  We have however many predictors $\mathscr{x}$, and one grouping variable $\mathscr{group}$ as an additional source of variance.

$$y = b_{\mathrm{intercept}} + b_{\mathrm{x_1}}\cdot \mathscr{x_1} + b_{\mathrm{x_2}}\cdot \mathscr{x_2}\ldots + \mathrm{effect}_{\mathscr{group}} + \epsilon$$

$$\mathrm{effect}_{\mathrm{group}} \sim \mathcal{N}(0, \tau^2)$$
$$\epsilon \sim \mathcal{N}(0, \sigma^2)$$

The above is the simplest model one would come across, often referred to as a random intercepts model, as the group effect is added to the intercept, and otherwise is a standard linear regression model.  

A concrete example would be something like where we have repeated observations of the same individuals over time.  The group or cluster in this case is the individual, and so we would have a person-specific effect in addition to the rest of the model effects.

We can expand this simple setting to include other grouping variables for random effects, and allow any of the coefficients to vary among those groups.  This covers a wide range of modeling situations, and in that case, a package like `lme4` is ideal.  It is fast, efficient, and many other packages expand on it.  As an example `lme4` syntax for a mixed model with random intercepts and slopes looks like the following.  Here the model regards the average reaction time per day for subjects in a sleep deprivation study.  We random effects for the intercept and coefficient/slope for the Days (i.e. time effect).

```{r lme4}
library(lme4)
model_lme4 = lmer(Reaction ~ Days + (1 + Days|Subject), sleepstudy)
summary(model_lme4)
```



However, there will be times where your data and model go beyond situations `lme4` is best suited for, and if so, there are additional tools that can aid you in your exploration.


### Goals

- Demonstrate other packages and their approach to mixed models
- Provide awareness of other possibilities for mixed models

### Prerequisites

You need to have these libraries for data sets, data processing, and visualization.

```{r preliminaries}
library(tidyverse)

# devtools::install_github('m-clark/noiris')
library(noiris)

# install.packages(ggeffects)
library(ggeffects)
```

Here will be some data sets we use.

```{r data}
recent_europe = gapminder_2019  %>% 
  filter(continent == 'Europe', year > 1950) %>% 
  mutate(year = year-min(year))


sleepstudy = lme4::sleepstudy
sleepstudy_trim =  sleepstudy %>% 
  filter(Days %in% c(0,4,9)) %>% 
  mutate(Days = factor(Days))
```


## glmmTMB {.tabset}

### Description

> Fit linear and generalized linear mixed models with various extensions,
including zero-inflation. The models are fitted using maximum likelihood
estimation via 'TMB' (Template Model Builder). 

- main function: `glmmTMB`
    

The `glmmTMB` package uses a different approach in estimating mixed models, which may be reason enough to use for some problems where `lme4` might struggle.  In general though, it tackles various kinds of mixed models that `lme4` is not designed to.
    
### Examples

#### Zero-inflated models

We can start with an example for zero-inflated models.  These are primarily count distribution models with an added component to deal with excess zeros.  The data regards counts of salamanders with site covariates and sampling covariates. Each of 23 sites was sampled 4 times.  We'll use the following variables:

- **site**: name of a location where repeated samples were taken

- **mined**: factor indicating whether the site was affected by mountain top removal coal mining

- **spp**: abbreviated species name, possibly also life stage

```{r fish}
library(glmmTMB)
Salamanders
```

We can see the zero spike visually.

```{r lotsofzeros, echo=FALSE}
Salamanders %>% 
  ggplot() +
  geom_bar(aes(x = count))
```


In `glmmTMB` we can do a zero-inflated negative binomial model to deal with excess zero counts.  Separate models are estimated for the count portion of the data and the binary portion (i.e. zeros vs non-zero). The predictors are species type and mining.  In general, the syntax is the same as `lme4`.  In this case we'll do a mixed model only for the count portion, but feel free to play with the model.

```{r zinb}
# Make mined more intuitive
Salamanders = Salamanders %>% 
  mutate(mined = fct_relevel(mined, 'no'))

model_zinb = glmmTMB(
  count ~ spp + mined + (1 | site),
  zi =  ~ spp + mined,
  family = nbinom2,
  data = Salamanders
)

summary(model_zinb)
```


In this case, the mining really has a say on the zero part of the model, where mining means a notably greater chance to have a zero count, but also fewer counts in general.  We can see that the expected count increases dramatically with no mining.

```{r zinb_plot}
ggpredict(model_zinb) %>% plot() # plot expected counts
```


#### Heterogeneous variances and autocorrelation

Sometimes we want to allow for heterogeneous variances or auto-correlated residuals.  The `glmmTMB` package has a specific syntax for this.  The following allows for different variance estimates at each time point.

```{r heterovar}
model_heterovar = glmmTMB(Reaction ~ Days + (1 | Subject) + diag(0 + Days | Subject), 
                          data = sleepstudy_trim)
summary(model_heterovar)
```


In this case, variance increases over time.  Note that the correlations are all zero.  glmmTMB can allow for such correlation using other functions rather than `diag`, such as autoregressive, spatial and more.  To be honest, you'll likely learn that many of these models are simply hard to fit, in which case, you might consder Bayesian methods.


```{r ar, eval=FALSE}
model_ar1 = glmmTMB(lifeExp ~ scale(year) + log(pop) + scale(giniPercap) + 
                               (1 | country) + 
                               ar1(0 + factor(year) | country), 
                             dispformula=~0,
                             data = gapminder_2019 %>% filter(year > 1980))
summary(model_ar1)
```

### Advantages

- Developed by one of the `lme4` authors
- More distributions
- Heterogeneous variance, residual correlation
- Model variance
- Nicely printed output

### Other

- Still in development for some areas


## Bayesian Approaches {.tabset}


### Description

#### rstanarm

> The `rstanarm` package is an appendage to the `rstan` package that enables many of the most common applied regression models to be estimated using Markov Chain Monte Carlo, variational approximations to the posterior distribution, or optimization. The `rstanarm` package allows these models to be specified using the customary R modeling syntax.

- main function: `stan_*`

#### brms

> The `brms` package provides an interface to fit Bayesian generalized multivariate (non-)linear multilevel models using `Stan`, which is a C++ package for obtaining full Bayesian inference (see http://mc-stan.org/). The formula syntax is an extended version of the syntax applied in the lme4 package to provide a familiar and simple interface for performing regression analyses.

- main function: `brm`


Using a Bayesian approach to mixed models means the sky is the limit in terms of model flexibility.  And once you get used to the Bayesian tools, much can be done with model exploration.


### Examples

Both brms and rstanarm stick to the lme4 approach as far as syntax is concerned.  So, unless you have a compelling reason not to, there is really no reason to not just do your mixed models as Bayesian ones, especially if you are doing more complicated models.  Note that one distinguishing feature of the Bayesian approach is that the random effects are estimated parameters of the model (*not* BLUPs). 


#### rstanarm

The `rstanarm` package not only uses the same syntax as `lme4`, you only have to attach the stan prefix to the `lmer` function. Bayesian methods are slower by definition, but for the following model it will only take a few seconds.  

```{r rstanarm, fig.width=3}
library(rstanarm)


model_rstanarm = stan_lmer(Reaction ~ Days + (Days | Subject),
                           sleepstudy,
                           cores = 4)  # for parallelization
                           
print(model_rstanarm, digits = 3)

qplot(
  data = ranef(model_rstanarm)$Subject,
  x = `(Intercept)`,
  y = Days,
  geom = c('point','smooth')
)
```



#### brms

The brms package by contrast has only one modeling function `brm`.  Again, the syntax is `lme4` style, but we can easily specify other alternatives.  Note that the additional flexibility requires brms to compile to Stan first, rather than using a prespecified model template.  However, even switching to 'robust' student t rather than normal distribution and adding autocorrelation still results in a model that only takes a few seconds after compilation.

```{r brms}
library(brms)
model_brm = brm(Reaction ~ Days + (Days | Subject),
                sleepstudy, 
                autocor = cor_ar(~Days|Subject),
                family = student,
                cores = 4)

summary(model_brm)
```

With the bayesian approach, we get nice summary, model comparison, and model diagnostic information as well.

```{r brms_summary}
ranef(model_brm)$Subject %>% apply(3, rbind)
marginal_effects(model_brm)
hypothesis(model_brm, 'Days > 10')    # is the Days coefficient greater than 10?
pp_check(model_brm)
pp_check(model_rstanarm, nreps = 10)
```

```{r compare_models}
model_brm_std = brm(Reaction ~ Days + (Days | Subject),
                    sleepstudy,
                    cores = 4)

WAIC(model_brm_std)    # quick check
WAIC(model_brm)
```


```{r compare_models2}
# a more statistical check
model_brm_std = add_criterion(model_brm_std, 'waic')
model_brm     = add_criterion(model_brm, 'waic')

# baseline model is the better one.
loo_compare(model_brm_std, model_brm, criterion = 'waic')  
```


### Advantages

#### rstanarm

- same lme4 syntax
- same lme4 function names
- no compilation time

#### brms

- same lme4 syntax
- heterogenous variances
- autocorrelation
- more distributions
- heterogeneous variance components
- multivariate models with correlated random effects
- multimembership models
- phylogenetic structures
- missing data imputation
- indirect effects

### Other

- Bayesian techniques are slower due to the way they are estimated, but for many situations not prohibitively so


## mgcv {.tabset}

### Description

> `mgcv` provides functions for generalized additive modelling (`gam` and `bam`) and generalized additive mixed modelling (`gamm`, and `random.effects`). The term GAM is taken to include any model dependent on unknown smooth functions of predictors and estimated by quadratically penalized (possibly quasi-) likelihood maximization. Available distributions are covered in `family.mgcv` and available smooths in `smooth.terms.`


- main function: `gam` (also `bam`)
- `gamm`
- `gamm4` (package)


Simply put, `mgcv` is one of the better modeling packages you'll come across.  One can use a general approach to go from complex models to simple group comparisons in a principled way, satisfying a great majority of modeling needs.  For our purposes here it's enough to note that there is a specific link between additive and mixed models such that they can be specified in the same way.  The benefit for applied users is that one can use the gam approach to do their mixed modeling.

### Examples



The following shows the basic mgcv approach to a GAM.

```{r mgcv_basic}
library(mgcv)
model_mgcv = gam(giniPercap ~ s(year), data = recent_europe)
summary(model_mgcv)
ggpredict(model_mgcv) %>% plot()
```

Lets do a random intercept model with a country random effect.

```{r mgcv_re}
recent_europe = recent_europe %>% 
  mutate(country = factor(country))
  
model_mgcv_re = gam(giniPercap ~ s(year) + s(country, bs='re'), 
                    data = recent_europe,
                    method = 'REML')
summary(model_mgcv_re)
gam.vcomp(model_mgcv_re)

```


```{r mgcv_slope}
model_mgcv_ran_slope = gam(giniPercap ~ year + s(country, bs='re') + s(year, country, bs='re'), 
                           data = recent_europe, 
                           method = 'REML')
summary(model_mgcv_ran_slope)
gam.vcomp(model_mgcv_ran_slope)
```

Compare to `lme4`

```{r mgcv_vs_lme4}
VarCorr(lmer(giniPercap ~ year  + (1 | country) + (0 + year | country), 
                    data = recent_europe))
```


### Advantages

- addition of smooth terms
    - same approach can be used with `brms`
- spatial random effects
- autocorrelation
- more distributions
- can use lme4 or nlme in conjunction
- seems to work better for glm
- much better for very large data glm
- bayesian prediction intervals
- better estimates of uncertainty for random effects


### Other

- Cannot estimate int-slope correlation


The mgcv package is one of the most powerful modeling tools in the R world in my opinion.  It can be used just for standard mixed models if desired.  It will be slower than lme4 for standard linear mixed models, but not prohibitively so, especially if you use bam.  If you go beyond such models, I've found it to be far faster than lme4 for very large glmm.

## Python Statsmodels {.tabset}

If you happen to be using Python, you don't have to switch to another tool to do mixed models.

### Description

> Statsmodels MixedLM handles most non-crossed random effects models, and some crossed models. To include crossed random effects in a model, it is necessary to treat the entire dataset as a single group. The variance components arguments to the model can then be used to define models with various combinations of crossed and non-crossed random effects.
> 
> The Statsmodels LME framework currently supports post-estimation inference via Wald tests and confidence intervals on the coefficients, profile likelihood analysis, likelihood ratio testing, and AIC.

### Examples


```{python statsmodels, eval = T, cache=FALSE}
import statsmodels.api as sm

import statsmodels.formula.api as smf

md = smf.mixedlm("Reaction ~ Days", data = r.sleepstudy, groups = r.sleepstudy["Subject"], re_formula="~Days")

mdf = md.fit()

print(mdf.summary())
```

```{python mixed}
import pandas as pd
mdf.conf_int()                                 # ci for fixed effects and variance components
pd.DataFrame.from_dict(mdf.random_effects)     # extract random effects
mdf.fe_params                                  # extract fixed effects
```


### Advantages

- Allows one to stay in the Python world if desired
- Clean/clear printout

### Other